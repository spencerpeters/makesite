Recently I came across OpenAI's awesome "Circuits Thread":
https://distill.pub/2020/circuits/
Some of the Circuits Thread team has since moved to Anthropic, where they released "Toy Models of Superposition":
https://transformer-circuits.pub/2022/toy_model/index.html

This line of research is focused on "Zooming in" and investigating the low-level details of neural networks. The Circuits Thread looks at a convolutional neural network vision model called InceptionV1 with about 10,000 unique neurons (up to translation). What they found completely upended my naive picture of deep learning.

I've never seen anyone explain a neural network's weights directly, so I assumed that this would be difficult or impossible. That the algorithms and representations learned by the neural network would not be localized to small regions of the network, such that individual neurons or weights would be totally meaningless, or at least, very difficult to understand and subject to interpretation.

But the Circuits thread shows that this isn't true at all! (A disclaimer here: I haven't read very much ML literature, I haven't personally done much ML, and probably a lot of these results and techniques have lots of precedent in the literature. They certainly don't *seem* very complicated! But the combined result makes such an impression on me that I'll enthuse over each technique as if it was totally  groundbreaking!)

The thread does a terrific job of visualizing parts of the network. To start with, each neuron is visualized by a synthetic image that was optimized, starting from random noise, to cause the neuron to activate maximally. These images are remarkably interpretable--you can see filters, lines, curves, boundary detectors, proto-dog-heads, and so on. You can see directly that boundary detector neurons use many different modalities to locate boundaries: color contrast, "frequency" contrast, the presence of a line with the correct orientation, and more.

After these images are used to classify neurons, it's not such a big step to analyze the weights between them. You can see that curve detectors are (largely) made out of early curve detectors, which are made out of line detectors, which are made out of Gabor filters. You can see this because the weights between them are positive. You can see that curve detectors are inhibited by early curve detectors in 180-degree opposing orientations, and if you artificially strip out that inhibition, the curve detectors will activate for a band of opposite orientations.

Reading through these experiments, I had two epiphanies. (Another disclaimer: these are the epiphanies of a skeptic! They're not new, but they're new for me.) Why does gradient descent work? Instead of thinking about some kind of abstract loss function landscape in high dimensions, I thought about a programming language where the inputs and outputs of all computations are just positive numbers--no complex data structures. The computation is structured linearly within each local step, so larger inputs contribute more to the next output, so data are naturally interpreted as magnitudes. (Think of it this way: the "convention" is that the output of a computation represents the presence or absence of something (like a curve), and its magnitude represents how strong the curve is, or how confident the computation is that it's seeing a curve.)
In this world, every part of the code can easily talk to every other part of the code. Moreover, it makes sense to tweak the weights locally. Is the thing I'm recognizing more present when a curve in a given orientation is present? Increase the weight a little bit and try again. That's gradient descent! Starting from nothing, beneficial feedback loops arise. Randomly, some neuron fires in response to furry textures in natural images. Upstream, the cat neuron trying to recognize a cat tweaks its weight to make use of that neuron. Now that the first neuron is involved in the cat computation, it updates its weights to previous neurons to be a slightly better fur detector. That updates its inputs' weights to be better line detectors, and so on.
(Ok, maybe this is painfully obvious to anyone who does any deep learning. But somehow it was never explained to me.)

The second epiphany: why are complex models like AlphaFold so damn effective at things like predicting how proteins fold? I'm embarrassed to admit that before reading the Circuits thread I'd categorized AlphaFold as "magic". In the sense of "sufficiently advanced technology". Maybe it had discovered, in some impossible-to-recognize way, the secret sauce in the structure of proteins designed by Nature that allows them to be efficiently redesigned by evolution and small mutations. Armed with the secret sauce, it could substantially simplify reasoning about how proteins fold. Maybe it did do this. But I now have a simpler, more direct picture of what is going on. It goes back to the old idea that ML models are better at engineering features than humans. The sophistication and multi-modality of the curve neurons in InceptionV1 drove this home for me. No doubt humans playing FoldIt can recognize fairly sophisticated patterns in protein sequences, and optimizations that can be applied to existing configurations. But ML models are better! The features they develop are richer and more precisely fitted to their target task than those that humans can derive. This extends from the low-level features like the affinities between different amino acid groups in different orientations, to high-level features like "beta sheets". Humans think they know what "beta sheets" are. But AlphaFold knows it more precisely, and has charted out the edge cases, exceptions, and deceptively similar distinct structures more thoroughly.

This recategorization from "magic" to "programming language with nice compositionality properties that enjoys beneficial feedback loops" has gotten me enthused about deep learning again. Maybe the visualization tools the Circuits thread develops could make actually training a model less miserable and opaque. Maybe I can start programming and debugging, rather than feeling like I'm writing code that flips bits in assembly and sees what sticks.
