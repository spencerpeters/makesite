<!-- title: Parsing -->
<!-- date: 2021-11-07 -->

I've been interested in better tooling for mathematical work for a long time. Programmers work in integrated development environments (IDEs) that understand the semantics of code. IDEs provide features like type checking, syntax highlighting, and autocomplete that completely transform the experience of programming. Math resembles code in many respects--in fact, there is even a [formal isomorphism](https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence) between them. But most mathematicians still do their work with pencil and paper. When they do use the computer to write up their results, they use a system ([LaTeX](https://www.latex-project.org/)) designed for typesetting, not for math. To be clear, I'm not talking about numerical calculations--computers have helped with those since the dawn of computing, and there are lots of great tools for numerical work. I'm talking about proving things, a category which includes symbolic calculations, "showing your work" in data analysis and other reasoning tasks, and mathematical research like my team's recent [hardness results](https://eccc.weizmann.ac.il/report/2021/141/). There are some tools for symbolic calculations (e.g., [Sympy](https://www.sympy.org/en/index.html) and [Mathematica](https://www.wolfram.com/mathematica/)), but in my experience they tend to be used as they were designed to be used, that is, for bashing out complicated calculations, rather than as full-fledged environments for mathematical reasoning (which IMO they don't support as well). There are also tools for proving things, called "proof assistants", which are very promising, but quite difficult to use (imagine having to explain to your IDE how your code works in excruciating detail before you can run it, or don't take my word for it, [try it out!](https://www.ma.imperial.ac.uk/~buzzard/xena/natural_number_game/)) The use cases for these tools are pretty narrow; yet I don't know of any more general-purpose tools in widespread use.

My dream is an editor that leverages knowledge of the semantics of math to provide an IDE-like experience that could 3X mathematical productivity. I'm talking effortless input via stylus or keyboard + full-featured autocomplete, type checking and syntax highlighting, context tooling like "jump to definition", tooltips, logical dependency trees, and "current goal", and computational tooling for producing numerics or plots, plus more that I can't think of yet. But the journey of a thousand miles begins with a single step. So this weekend, I was reading about parsing (this post was supposed to be about parsing!) so that I could prototype a very simple "first step", namely, adding semantic annotations like "variable declarations" to LaTeX a la TypeScript. (Since most mathematicians, including me, have to write up their work in LaTeX, this seems like a natural way to get started.)

It's pretty shocking that some preliminary Googling didn't turn up any articles surveying different parsing technologies and their relative advantages and disadvantages. At the risk of great simplification (this entire post runs that risk!) there are two parsing technologies that a computer science PhD like me who doesn't specialize in programming languages has heard of. First, there's [recursive descent parsing](https://en.wikipedia.org/wiki/Recursive_descent_parser). This approach is probably what you would take if I asked you to write a parser from scratch. You write a set of functions which call each other recursively, where each function corresponds to some part of the language's syntax. Due to its simplicity, this is a pretty fuzzy class--I'll explain more details a bit later.  Second, there are parsers for a specific class of languages, those specified by [context-free grammars](https://en.wikipedia.org/wiki/Context-free_grammar). These parsers are typically constructed automatically, just like functions that match regular expressions are constructed automatically. In fact, the computations that parse context-free grammars are a more complicated (and more powerful) variant of the computations that match regular expressions; the latter are [finite automata](https://en.wikipedia.org/wiki/Deterministic_finite_automaton), while the former are [pushdown automata](https://en.wikipedia.org/wiki/Pushdown_automaton). In short, for these parsers, you input a formal language specification to lexer and parser generator tools like [lex and yacc](https://en.wikipedia.org/wiki/Lex_(software)), and you automatically get a parser as output. The upside is that you only need to write a formal language specification, but the downside is that you have to write a formal language specification. Here is where my understanding gets a little strained. This strategy doesn't work for all context-free grammars. Parsing proceeds from left to right. TO BE CONTINUED

As the parser tries to match what it's seeing against the language specification, sometimes it needs to back up an

In contrast, in recursive descent parsing, y
